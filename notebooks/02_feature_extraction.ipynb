{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "429bf651",
   "metadata": {},
   "source": [
    "# Visual Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7aba53",
   "metadata": {},
   "source": [
    "mp4 -> fps -> npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d5d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat model ResNet50...\n",
      "✅ Model Visual (ResNet50) siap.\n",
      "Semua path dan folder output sudah siap.\n",
      "\n",
      "--- TAHAP 1: Memulai ekstraksi fitur visual satu per satu ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ekstrak Visual dari train: 100%|██████████| 802/802 [51:31<00:00,  3.86s/it]\n",
      "Ekstrak Visual dari test: 100%|██████████| 200/200 [13:03<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ekstraksi individual selesai.\n",
      "\n",
      "--- TAHAP 2: Menggabungkan semua hasil fitur ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Menggabungkan fitur dari temp_visual_train: 100%|██████████| 802/802 [00:12<00:00, 62.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur final disimpan di visual_train.npy dengan bentuk: (802, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Menggabungkan fitur dari temp_visual_test: 100%|██████████| 200/200 [00:02<00:00, 69.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur final disimpan di visual_test.npy dengan bentuk: (200, 2048)\n",
      "\n",
      "\n",
      "SELESAI! ✅ Fitur visual siap digunakan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm # Pakai versi notebook biar keren di Jupyter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. SETUP MODEL & PATH\n",
    "print(\"Memuat model ResNet50...\")\n",
    "# Buat base model \n",
    "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "# Definisikan model dari base model yang sama\n",
    "visual_model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "IMG_SIZE = 224\n",
    "print(\"✅ Model Visual (ResNet50) siap.\")\n",
    "\n",
    "# --- Path Setup ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "VIDEO_DIR = DATA_DIR / \"video\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "FEATURES_DIR = BASE_DIR / \"features\"\n",
    "\n",
    "# Path folder input video\n",
    "TRAIN_VIDEO_DIR = VIDEO_DIR / \"train\"\n",
    "TEST_VIDEO_DIR = VIDEO_DIR / \"test\"\n",
    "\n",
    "# Path folder output sementara untuk menyimpan progres\n",
    "TEMP_VISUAL_TRAIN_DIR = FEATURES_DIR / \"temp_visual_train\"\n",
    "TEMP_VISUAL_TEST_DIR = FEATURES_DIR / \"temp_visual_test\"\n",
    "\n",
    "# Path file CSV\n",
    "TRAIN_CSV_PATH = PROCESSED_DATA_DIR / \"train_clean.csv\"\n",
    "TEST_CSV_PATH = PROCESSED_DATA_DIR / \"test_clean.csv\"\n",
    "\n",
    "# Path file output final\n",
    "TRAIN_VISUAL_FEATURES_PATH = FEATURES_DIR / \"visual_train.npy\"\n",
    "TEST_VISUAL_FEATURES_PATH = FEATURES_DIR / \"visual_test.npy\"\n",
    "\n",
    "# Membuat semua folder output yang dibutuhkan\n",
    "FEATURES_DIR.mkdir(exist_ok=True)\n",
    "TEMP_VISUAL_TRAIN_DIR.mkdir(exist_ok=True)\n",
    "TEMP_VISUAL_TEST_DIR.mkdir(exist_ok=True)\n",
    "print(\"Semua path dan folder output sudah siap.\")\n",
    "\n",
    "# 2. FUNGSI EKSTRAKSI \n",
    "def extract_visual_features(video_path, num_frames=30):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames < 1: return np.zeros(2048)\n",
    "    \n",
    "    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "    frames = []\n",
    "    for i in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    if not frames: return np.zeros(2048)\n",
    "    \n",
    "    # Proses dengan batching kecil untuk hemat memori\n",
    "    features = visual_model.predict(preprocess_input(np.array(frames)), batch_size=8, verbose=0)\n",
    "    return np.mean(features, axis=0)\n",
    "\n",
    "# 3. TAHAP 1: EKSTRAK & SIMPAN PROGRES PER FILE\n",
    "def process_and_save_individually(csv_path, video_dir, output_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for video_id in tqdm(df['id'], desc=f\"Ekstrak Visual dari {video_dir.name}\"):\n",
    "        video_path = video_dir / f\"{video_id}.mp4\"\n",
    "        feature_path = output_dir / f\"{video_id}.npy\"\n",
    "        \n",
    "        # Cek apakah file ini sudah pernah diproses, jika iya, lewati\n",
    "        if feature_path.exists():\n",
    "            continue\n",
    "            \n",
    "        if not video_path.exists():\n",
    "            features = np.zeros(2048) # Output ResNet50\n",
    "        else:\n",
    "            features = extract_visual_features(video_path)\n",
    "        \n",
    "        np.save(feature_path, features)\n",
    "\n",
    "print(\"\\n--- TAHAP 1: Memulai ekstraksi fitur visual satu per satu ---\")\n",
    "process_and_save_individually(TRAIN_CSV_PATH, TRAIN_VIDEO_DIR, TEMP_VISUAL_TRAIN_DIR)\n",
    "process_and_save_individually(TEST_CSV_PATH, TEST_VIDEO_DIR, TEMP_VISUAL_TEST_DIR)\n",
    "print(\"✅ Ekstraksi individual selesai.\")\n",
    "\n",
    "# 4. TAHAP 2: GABUNGKAN SEMUA HASIL\n",
    "def combine_features(csv_path, temp_dir, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    feature_list = []\n",
    "    for video_id in tqdm(df['id'], desc=f\"Menggabungkan fitur dari {temp_dir.name}\"):\n",
    "        feature = np.load(temp_dir / f\"{video_id}.npy\")\n",
    "        feature_list.append(feature)\n",
    "    \n",
    "    final_features = np.array(feature_list)\n",
    "    np.save(output_path, final_features)\n",
    "    print(f\"Fitur final disimpan di {output_path.name} dengan bentuk: {final_features.shape}\")\n",
    "\n",
    "print(\"\\n--- TAHAP 2: Menggabungkan semua hasil fitur ---\")\n",
    "combine_features(TRAIN_CSV_PATH, TEMP_VISUAL_TRAIN_DIR, TRAIN_VISUAL_FEATURES_PATH)\n",
    "combine_features(TEST_CSV_PATH, TEMP_VISUAL_TEST_DIR, TEST_VISUAL_FEATURES_PATH)\n",
    "\n",
    "print(\"\\n\\nSELESAI! ✅ Fitur visual siap digunakan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a651239",
   "metadata": {},
   "source": [
    "# Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edacebb",
   "metadata": {},
   "source": [
    "mp4 -> txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb34a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat model Whisper...\n",
      "✅ Model Teks (Whisper) siap.\n",
      "Semua path dan folder output sudah siap.\n",
      "\n",
      "--- TAHAP 1: Memulai transkripsi audio ke teks satu per satu ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transkripsi dari train: 100%|██████████| 802/802 [2:56:16<00:00, 13.19s/it]\n",
      "Transkripsi dari test: 100%|██████████| 200/200 [45:49<00:00, 13.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transkripsi individual selesai.\n",
      "\n",
      "--- TAHAP 2: Menggabungkan semua hasil transkrip ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Menggabungkan transkrip dari temp_text_train: 100%|██████████| 802/802 [00:11<00:00, 69.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transkrip final disimpan di text_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Menggabungkan transkrip dari temp_text_test: 100%|██████████| 200/200 [00:02<00:00, 75.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transkrip final disimpan di text_test.csv\n",
      "\n",
      "\n",
      "SELESAI! ✅ Transkrip teks siap digunakan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import whisper\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. SETUP MODEL & PATH\n",
    "print(\"Memuat model Whisper...\")\n",
    "try:\n",
    "    model = whisper.load_model(\"base\")\n",
    "    print(\"✅ Model Teks (Whisper) siap.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Gagal memuat model Whisper: {e}\")\n",
    "\n",
    "# --- Path Setup ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "VIDEO_DIR = DATA_DIR / \"video\" \n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "FEATURES_DIR = BASE_DIR / \"features\"\n",
    "\n",
    "# Path folder input video\n",
    "TRAIN_VIDEO_DIR = VIDEO_DIR / \"train\"\n",
    "TEST_VIDEO_DIR = VIDEO_DIR / \"test\"\n",
    "\n",
    "# Path folder output sementara untuk menyimpan transkrip per file\n",
    "TEMP_TEXT_TRAIN_DIR = FEATURES_DIR / \"temp_text_train\"\n",
    "TEMP_TEXT_TEST_DIR = FEATURES_DIR / \"temp_text_test\"\n",
    "\n",
    "# Path file CSV\n",
    "TRAIN_CSV_PATH = PROCESSED_DATA_DIR / \"train_clean.csv\"\n",
    "TEST_CSV_PATH = PROCESSED_DATA_DIR / \"test_clean.csv\"\n",
    "\n",
    "# Path file output final \n",
    "TRAIN_TEXT_FEATURES_PATH = FEATURES_DIR / \"text_train.csv\"\n",
    "TEST_TEXT_FEATURES_PATH = FEATURES_DIR / \"text_test.csv\"\n",
    "\n",
    "# Membuat semua folder output yang dibutuhkan\n",
    "FEATURES_DIR.mkdir(exist_ok=True)\n",
    "TEMP_TEXT_TRAIN_DIR.mkdir(exist_ok=True)\n",
    "TEMP_TEXT_TEST_DIR.mkdir(exist_ok=True)\n",
    "print(\"Semua path dan folder output sudah siap.\")\n",
    "\n",
    "# 2. TAHAP 1: TRANSKRIPSI & SIMPAN PROGRES PER FILE\n",
    "def transcribe_and_save_individually(csv_path, video_dir, output_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for video_id in tqdm(df['id'], desc=f\"Transkripsi dari {video_dir.name}\"):\n",
    "        video_path = video_dir / f\"{video_id}.mp4\"\n",
    "        # Kita simpan hasilnya sebagai file teks biasa (.txt)\n",
    "        transcript_path = output_dir / f\"{video_id}.txt\"\n",
    "        \n",
    "        # Cek apakah file ini sudah pernah diproses, jika iya, lewati\n",
    "        if transcript_path.exists():\n",
    "            continue\n",
    "            \n",
    "        transcript_text = \"\"\n",
    "        if video_path.exists():\n",
    "            try:\n",
    "                # Lakukan transkripsi\n",
    "                result = model.transcribe(str(video_path), fp16=False)\n",
    "                transcript_text = result['text']\n",
    "            except Exception as e:\n",
    "                print(f\"Gagal transkripsi {video_path.name}: {e}\")\n",
    "                transcript_text = \"\" # Beri teks kosong jika gagal\n",
    "        \n",
    "        # Simpan hasil transkrip ke file .txt\n",
    "        with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(transcript_text)\n",
    "\n",
    "print(\"\\n--- TAHAP 1: Memulai transkripsi audio ke teks satu per satu ---\")\n",
    "transcribe_and_save_individually(TRAIN_CSV_PATH, TRAIN_VIDEO_DIR, TEMP_TEXT_TRAIN_DIR)\n",
    "transcribe_and_save_individually(TEST_CSV_PATH, TEST_VIDEO_DIR, TEMP_TEXT_TEST_DIR)\n",
    "print(\"✅ Transkripsi individual selesai.\")\n",
    "\n",
    "# 3. TAHAP 2: GABUNGKAN SEMUA HASIL TRANSKRIP\n",
    "def combine_transcripts(csv_path, temp_dir, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    transcripts = []\n",
    "    for video_id in tqdm(df['id'], desc=f\"Menggabungkan transkrip dari {temp_dir.name}\"):\n",
    "        transcript_path = temp_dir / f\"{video_id}.txt\"\n",
    "        try:\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        except FileNotFoundError:\n",
    "            text = \"\" # Teks kosong jika file tidak ditemukan\n",
    "        transcripts.append(text)\n",
    "    \n",
    "    # Buat DataFrame baru berisi id dan transkrip\n",
    "    result_df = pd.DataFrame({\n",
    "        'id': df['id'],\n",
    "        'transcript': transcripts\n",
    "    })\n",
    "    \n",
    "    # Simpan ke file CSV\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Transkrip final disimpan di {output_path.name}\")\n",
    "\n",
    "print(\"\\n--- TAHAP 2: Menggabungkan semua hasil transkrip ---\")\n",
    "combine_transcripts(TRAIN_CSV_PATH, TEMP_TEXT_TRAIN_DIR, TRAIN_TEXT_FEATURES_PATH)\n",
    "combine_transcripts(TEST_CSV_PATH, TEMP_TEXT_TEST_DIR, TEST_TEXT_FEATURES_PATH)\n",
    "\n",
    "print(\"\\n\\nSELESAI! ✅ Transkrip teks siap digunakan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc49c52",
   "metadata": {},
   "source": [
    "Ekstraksi Fitur Teks (IndoBERT Embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285394d",
   "metadata": {},
   "source": [
    "txt -> npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc224781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kn409\\anaconda3\\envs\\satria_gpu\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\kn409\\anaconda3\\envs\\satria_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengecek ketersediaan GPU untuk PyTorch...\n",
      "Menggunakan device: cpu\n",
      "\n",
      "Memuat model IndoBERT...\n",
      "✅ Model IndoBERT siap.\n",
      "Semua path sudah siap.\n",
      "\n",
      "--- Memulai proses embedding untuk data latih ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membuat embedding dari text_train.csv: 100%|██████████| 802/802 [01:55<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur teks final disimpan di text_train.npy dengan bentuk: (802, 768)\n",
      "\n",
      "--- Memulai proses embedding untuk data tes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membuat embedding dari text_test.csv: 100%|██████████| 200/200 [00:30<00:00,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur teks final disimpan di text_test.npy dengan bentuk: (200, 768)\n",
      "\n",
      "\n",
      "SELESAI! ✅ Semua fitur (audio, visual, teks) sudah lengkap dan siap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. SETUP MODEL INDOBERT & PATH\n",
    "print(\"Mengecek ketersediaan GPU untuk PyTorch...\")\n",
    "# Cek apakah GPU bisa digunakan oleh PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Menggunakan device: {device}\")\n",
    "\n",
    "print(\"\\nMemuat model IndoBERT...\")\n",
    "try:\n",
    "    # Muat tokenizer dan model IndoBERT\n",
    "    tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "    model = BertModel.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "    # Pindahkan model ke GPU jika tersedia\n",
    "    model.to(device)\n",
    "    print(\"✅ Model IndoBERT siap.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Gagal memuat model IndoBERT: {e}\")\n",
    "\n",
    "# --- Path Setup ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "FEATURES_DIR = BASE_DIR / \"features\"\n",
    "\n",
    "# Path file input (hasil dari Whisper)\n",
    "TRAIN_TEXT_CSV_PATH = FEATURES_DIR / \"text_train.csv\"\n",
    "TEST_TEXT_CSV_PATH = FEATURES_DIR / \"text_test.csv\"\n",
    "\n",
    "# Path file output final (.npy)\n",
    "TRAIN_TEXT_FEATURES_PATH = FEATURES_DIR / \"text_train.npy\"\n",
    "TEST_TEXT_FEATURES_PATH = FEATURES_DIR / \"text_test.npy\"\n",
    "\n",
    "print(\"Semua path sudah siap.\")\n",
    "\n",
    "# 2. FUNGSI UNTUK MENDAPATKAN EMBEDDING\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"\n",
    "    Mengubah satu kalimat teks menjadi satu vektor fitur (embedding).\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        # IndoBERT base menghasilkan 768 fitur\n",
    "        return np.zeros(768) \n",
    "\n",
    "    # Tokenisasi teks\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128, padding=True)\n",
    "    # Pindahkan input tensor ke GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Dapatkan output dari model tanpa menghitung gradien (lebih hemat memori)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Ambil embedding dari token [CLS] (representasi keseluruhan kalimat)\n",
    "    embedding = outputs.last_hidden_state[0, 0, :].cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# 3. PROSES KESELURUHAN DATASET\n",
    "def process_text_dataset(csv_path, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for text in tqdm(df['transcript'], desc=f\"Membuat embedding dari {csv_path.name}\"):\n",
    "        embedding = get_text_embedding(text)\n",
    "        all_embeddings.append(embedding)\n",
    "    \n",
    "    # Simpan hasil akhir ke file .npy\n",
    "    final_embeddings = np.array(all_embeddings)\n",
    "    np.save(output_path, final_embeddings)\n",
    "    print(f\"Fitur teks final disimpan di {output_path.name} dengan bentuk: {final_embeddings.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Memulai proses embedding untuk data latih ---\")\n",
    "process_text_dataset(TRAIN_TEXT_CSV_PATH, TRAIN_TEXT_FEATURES_PATH)\n",
    "\n",
    "print(\"\\n--- Memulai proses embedding untuk data tes ---\")\n",
    "process_text_dataset(TEST_TEXT_CSV_PATH, TEST_TEXT_FEATURES_PATH)\n",
    "\n",
    "print(\"\\n\\nSELESAI! ✅ Semua fitur (audio, visual, teks) sudah lengkap dan siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a49a1",
   "metadata": {},
   "source": [
    "# Audio Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba17ae2",
   "metadata": {},
   "source": [
    "mp4 -> wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai proses konversi MP4 ke WAV...\n",
      "\n",
      "Memproses folder: c:\\Caelan\\BDC2025\\data\\video\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mengonversi train: 100%|██████████| 775/775 [05:25<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memproses folder: c:\\Caelan\\BDC2025\\data\\video\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mengonversi test: 100%|██████████| 198/198 [01:21<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Semua video berhasil dikonversi ke format WAV! 🚀\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Konfigurasi Path ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "VIDEO_DIR = DATA_DIR / \"video\"\n",
    "WAV_DIR = DATA_DIR / \"wav\" # Folder output untuk file .wav\n",
    "\n",
    "# List direktori yang akan diproses\n",
    "video_subdirs = [VIDEO_DIR / \"train\", VIDEO_DIR / \"test\"]\n",
    "\n",
    "print(\"Memulai proses konversi MP4 ke WAV...\")\n",
    "\n",
    "# Loop untuk folder train dan test\n",
    "for video_subdir in video_subdirs:\n",
    "    # Tentukan folder output untuk file .wav\n",
    "    wav_output_dir = WAV_DIR / video_subdir.name\n",
    "    # Buat folder output jika belum ada\n",
    "    wav_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nMemproses folder: {video_subdir}\")\n",
    "    \n",
    "    # Dapatkan daftar semua file .mp4 di dalam subdirektori\n",
    "    video_files = list(video_subdir.glob(\"*.mp4\"))\n",
    "    \n",
    "    for video_path in tqdm(video_files, desc=f\"Mengonversi {video_subdir.name}\"):\n",
    "        video_id = video_path.stem # Mengambil nama file tanpa ekstensi (misal: \"1\")\n",
    "        wav_path = wav_output_dir / f\"{video_id}.wav\"\n",
    "        \n",
    "        # Lewati file yang sudah pernah dikonversi\n",
    "        if wav_path.exists():\n",
    "            continue\n",
    "            \n",
    "        # Perintah FFMPEG untuk konversi\n",
    "        # -i: input file\n",
    "        # -vn: abaikan video (kita hanya butuh audio)\n",
    "        # -acodec pcm_s16le: format audio standar untuk .wav\n",
    "        # -ar 22050: sample rate 22.050 Hz\n",
    "        # -ac 1: mono channel\n",
    "        command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-vn\",\n",
    "            \"-acodec\", \"pcm_s16le\",\n",
    "            \"-ar\", \"22050\",\n",
    "            \"-ac\", \"1\",\n",
    "            str(wav_path),\n",
    "            \"-hide_banner\", # Sembunyikan info banner ffmpeg\n",
    "            \"-loglevel\", \"error\" # Hanya tampilkan error jika ada\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Menjalankan perintah ffmpeg\n",
    "            subprocess.run(command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Gagal mengonversi {video_path.name}: {e}\")\n",
    "\n",
    "print(\"\\n\\nSemua video berhasil dikonversi ke format WAV! 🚀\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a9571",
   "metadata": {},
   "source": [
    "diagnostik librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "MEMULAI TES DIAGNOSTIK LIBROSA\n",
      "========================================\n",
      "Mencoba memuat file: c:\\Caelan\\BDC2025\\data\\wav\\train\\1.wav\n",
      "Apakah file ada? -> True\n",
      "\n",
      "✅ BERHASIL! Audio berhasil dimuat tanpa crash.\n",
      "   Sample rate: 22050\n",
      "   Jumlah sampel audio: 2609214\n",
      "\n",
      "Tes diagnostik selesai.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from pathlib import Path\n",
    "\n",
    "file_tes = Path.cwd().parent / \"data\" / \"wav\" / \"train\" / \"1.wav\"\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"MEMULAI TES DIAGNOSTIK LIBROSA\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Mencoba memuat file: {file_tes}\")\n",
    "print(f\"Apakah file ada? -> {file_tes.exists()}\")\n",
    "\n",
    "try:\n",
    "    # Ini adalah perintah paling dasar untuk memuat audio.\n",
    "    # Jika ini crash, masalah ada di librosa/environment.\n",
    "    y, sr = librosa.load(file_tes)\n",
    "    \n",
    "    print(\"\\n✅ BERHASIL! Audio berhasil dimuat tanpa crash.\")\n",
    "    print(f\"   Sample rate: {sr}\")\n",
    "    print(f\"   Jumlah sampel audio: {len(y)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ GAGAL dengan error Python: {e}\")\n",
    "\n",
    "print(\"\\nTes diagnostik selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2381c4c",
   "metadata": {},
   "source": [
    "wav -> npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b6ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semua path dan folder output sudah siap.\n",
      "\n",
      "--- TAHAP 1: Memulai ekstraksi fitur satu per satu ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ekstrak & Simpan dari train: 100%|██████████| 802/802 [01:42<00:00,  7.80it/s]\n",
      "Ekstrak & Simpan dari test: 100%|██████████| 200/200 [00:24<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ekstraksi individual selesai.\n",
      "\n",
      "--- TAHAP 2: Menggabungkan semua hasil fitur ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Menggabungkan fitur dari temp_train: 100%|██████████| 802/802 [00:08<00:00, 96.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur training final disimpan di c:\\Caelan\\BDC2025\\features\\audio_train.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Menggabungkan fitur dari temp_test: 100%|██████████| 200/200 [00:02<00:00, 97.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur testing final disimpan di c:\\Caelan\\BDC2025\\features\\audio_test.npy\n",
      "\n",
      "--- TAHAP 3: Membuat file label ---\n",
      "Label training berhasil disimpan di: c:\\Caelan\\BDC2025\\features\\audio_labels_train.npy\n",
      "\n",
      "\n",
      "SELESAI TOTAL! ✅ Semua file siap digunakan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Abaikan warning dari librosa yang tidak relevan\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# 1. SETUP PATH\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "WAV_DIR = DATA_DIR / \"wav\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "FEATURES_DIR = BASE_DIR / \"features\"\n",
    "\n",
    "# Path folder input WAV\n",
    "TRAIN_WAV_DIR = WAV_DIR / \"train\"\n",
    "TEST_WAV_DIR = WAV_DIR / \"test\"\n",
    "\n",
    "# Path folder output sementara untuk .npy per file\n",
    "TEMP_TRAIN_DIR = FEATURES_DIR / \"temp_train\"\n",
    "TEMP_TEST_DIR = FEATURES_DIR / \"temp_test\"\n",
    "\n",
    "# Path file CSV\n",
    "TRAIN_CSV_PATH = PROCESSED_DATA_DIR / \"train_clean.csv\"\n",
    "TEST_CSV_PATH = PROCESSED_DATA_DIR / \"test_clean.csv\"\n",
    "\n",
    "# Path file output final\n",
    "TRAIN_FEATURES_PATH = FEATURES_DIR / \"audio_train.npy\"\n",
    "TEST_FEATURES_PATH = FEATURES_DIR / \"audio_test.npy\"\n",
    "TRAIN_LABELS_PATH = FEATURES_DIR / \"audio_labels_train.npy\"\n",
    "\n",
    "# Membuat semua folder output yang dibutuhkan\n",
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEMP_TRAIN_DIR.mkdir(exist_ok=True)\n",
    "TEMP_TEST_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Semua path dan folder output sudah siap.\")\n",
    "\n",
    "# 2. FUNGSI EKSTRAKSI \n",
    "def extract_audio_features_from_wav(wav_path, n_mfcc=20):\n",
    "    try:\n",
    "        y, sr = librosa.load(wav_path)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        return np.mean(mfccs.T, axis=0)\n",
    "    except Exception:\n",
    "        return np.zeros(n_mfcc)\n",
    "\n",
    "# TAHAP 1: EKSTRAK FITUR SECARA INDIVIDUAL (HEMAT MEMORI)\n",
    "def process_and_save_individually(csv_path, wav_dir, output_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for video_id in tqdm(df['id'], desc=f\"Ekstrak & Simpan dari {wav_dir.name}\"):\n",
    "        wav_path = wav_dir / f\"{video_id}.wav\"\n",
    "        feature_path = output_dir / f\"{video_id}.npy\"\n",
    "        \n",
    "        if feature_path.exists():\n",
    "            continue\n",
    "            \n",
    "        if not wav_path.exists():\n",
    "            features = np.zeros(20)\n",
    "        else:\n",
    "            features = extract_audio_features_from_wav(wav_path)\n",
    "        \n",
    "        np.save(feature_path, features)\n",
    "\n",
    "print(\"\\n--- TAHAP 1: Memulai ekstraksi fitur satu per satu ---\")\n",
    "process_and_save_individually(TRAIN_CSV_PATH, TRAIN_WAV_DIR, TEMP_TRAIN_DIR)\n",
    "process_and_save_individually(TEST_CSV_PATH, TEST_WAV_DIR, TEMP_TEST_DIR)\n",
    "print(\"Ekstraksi individual selesai.\")\n",
    "\n",
    "# TAHAP 2: GABUNGKAN SEMUA FILE FITUR\n",
    "def combine_features(csv_path, temp_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    feature_list = []\n",
    "    for video_id in tqdm(df['id'], desc=f\"Menggabungkan fitur dari {temp_dir.name}\"):\n",
    "        feature = np.load(temp_dir / f\"{video_id}.npy\")\n",
    "        feature_list.append(feature)\n",
    "    return np.array(feature_list)\n",
    "\n",
    "print(\"\\n--- TAHAP 2: Menggabungkan semua hasil fitur ---\")\n",
    "# Gabungkan & simpan fitur train\n",
    "train_features = combine_features(TRAIN_CSV_PATH, TEMP_TRAIN_DIR)\n",
    "np.save(TRAIN_FEATURES_PATH, train_features)\n",
    "print(f\"Fitur training final disimpan di {TRAIN_FEATURES_PATH}\")\n",
    "\n",
    "# Gabungkan & simpan fitur test\n",
    "test_features = combine_features(TEST_CSV_PATH, TEMP_TEST_DIR)\n",
    "np.save(TEST_FEATURES_PATH, test_features)\n",
    "print(f\"Fitur testing final disimpan di {TEST_FEATURES_PATH}\")\n",
    "\n",
    "# TAHAP 3: BUAT FILE LABEL (Sama seperti sebelumnya)\n",
    "print(\"\\n--- TAHAP 3: Membuat file label ---\")\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "emotion_mapping = {\n",
    "    'Proud': 0, 'Trust': 1, 'Joy': 2, 'Surprise': 3,\n",
    "    'Neutral': 4, 'Sadness': 5, 'Fear': 6, 'Anger': 7\n",
    "}\n",
    "train_df['emotion_code'] = train_df['emotion'].map(emotion_mapping).fillna(train_df['emotion']).astype(int)\n",
    "train_labels_np = train_df['emotion_code'].to_numpy()\n",
    "np.save(TRAIN_LABELS_PATH, train_labels_np)\n",
    "print(f\"Label training berhasil disimpan di: {TRAIN_LABELS_PATH}\")\n",
    "\n",
    "print(\"\\n\\nSELESAI TOTAL! ✅ Semua file siap digunakan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede09bf",
   "metadata": {},
   "source": [
    "### Output Ekstraksi Fitur\n",
    "\n",
    "* **Visual**: `mp4 → fps → npy`\n",
    "* **Text**: `mp4 → txt → npy`\n",
    "* **Audio**: `mp4 → wav → npy`\n",
    "* Semua hasil ekstraksi disimpan di folder **`features/`**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satria_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
